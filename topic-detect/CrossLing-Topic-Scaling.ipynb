{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, nltk, string, os, gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# this represent any text as a single \"doc-embedding\" we use it both for the query and the sentences\n",
    "# input should be a string\n",
    "def text_embedding(text,lang):\n",
    "    \n",
    "    lang = lang.lower()\n",
    "    \n",
    "    #you should check in the embeddings you use if the words have been lowercased or not. \n",
    "    #try ask the embedding for \"barack\" and for \"Barack\"\n",
    "    # if the Barack works, then comment the following line\n",
    "    text = text.lower()\n",
    "    \n",
    "    # we tokenize the text in single words\n",
    "    text = nltk.tokenize.WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    # we remove numbers and punctuation\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    \n",
    "    doc_embed = []\n",
    "    \n",
    "    # for each word we get the embedding and we append it to a list\n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = emb_model[lang+\"__\"+word]\n",
    "                doc_embed.append(embed_word)\n",
    "            except KeyError:\n",
    "#                print (\"not found:\", word)\n",
    "                continue\n",
    "    # we average the embeddings of all the words, getting an overall doc embedding\n",
    "    if len(doc_embed)>0:\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*doc_embed)]\n",
    "\n",
    "        avg = np.array(avg).reshape(1, -1)\n",
    "\n",
    "        # the output is a doc-embedding\n",
    "        return avg\n",
    "    else:\n",
    "        return \"Empty\"\n",
    "    \n",
    "def clean(text):\n",
    "    text = text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = gensim.models.KeyedVectors.load_word2vec_format('../../TextScaling/topfish/edited.wiki.big-five.mapped.vec', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found: herrschend\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"elite politiker establishment herrschend\"\n",
    "\n",
    "lang = \"DE\"\n",
    "\n",
    "res = emb_model.wv.most_similar(positive=['de__volk'], topn=50)\n",
    "\n",
    "\n",
    "query_emb = text_embedding(query, lang)\n",
    "\n",
    "# add the path to the folder where you have your manifestos as text documents\n",
    "# collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/deu2017/\"\n",
    "\n",
    "# only Germany for less output \n",
    "#collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/germany17/\"\n",
    "#collection_path = \"../resources/deu2017/\"\n",
    "\n",
    "collection_path = \"/Users/federiconanni/Dropbox/University/research/sparserhetoric/polidoc_bigfive_longitude/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42420.000.2013.1.1.txt DE\n",
      "41950.000.2009.1.1.txt DE\n",
      "53202.000.2016.1.1.txt EN\n",
      "53320.000.2007.1.1.txt EN\n",
      "43530.000.2007.1.1.txt DE\n",
      "51210.000.2010.1.1.txt EN\n",
      "53110.000.2011.1.1.txt EN\n",
      "51401.000.2010.1.1.txt EN\n",
      "41420.000.2013.1.1.txt DE\n",
      "51320.000.2017.1.1.txt EN\n",
      "53951.000.2011.1.1.txt EN\n",
      "42520.000.2017.1.1.txt DE\n",
      "43810.000.2015.1.1.txt DE\n",
      "42110.000.2013.1.1.txt DE\n",
      "41113.000.2017.1.1.txt DE\n",
      "43520.000.2007.1.1.txt DE\n",
      "51620.000.2015.1.1.txt EN\n",
      "53206.000.2016.1.1.txt EN\n",
      "42421.000.2008.1.1.txt DE\n",
      "51902.000.2015.1.1.txt EN\n",
      "43320.000.2015.1.1.txt DE\n",
      "53320.000.2016.1.1.txt EN\n",
      "53621.000.2016.1.1.txt EN\n",
      "43531.000.2007.1.1.txt DE\n",
      "53620.000.2007.1.1.txt EN\n",
      "43110.000.2011.1.1.txt DE\n",
      "53520.000.2016.1.1.txt EN\n",
      "43520.000.2015.1.1.txt DE\n",
      "53220.000.2011.1.1.txt EN\n",
      "41950.000.2013.1.1.txt DE\n",
      "43810.000.2007.1.1.txt DE\n",
      "41320.000.2017.1.1.txt DE\n",
      "42320.000.2017.1.1.txt DE\n",
      "41953.000.2017.1.1.txt DE\n",
      "42450.000.2013.1.1.txt DE\n",
      "51421.000.2010.1.1.txt EN\n",
      "51620.000.2017.1.1.txt EN\n",
      "42110.000.2008.1.1.txt DE\n",
      "51902.000.2017.1.1.txt EN\n",
      "41420.000.2009.1.1.txt DE\n",
      "51101.000.2010.1.1.txt EN\n",
      "43320.000.2007.1.1.txt DE\n",
      "42951.000.2008.1.1.txt DE\n",
      "43420.000.2011.1.1.txt DE\n",
      "43530.000.2015.1.1.txt DE\n",
      "53321.000.2016.1.1.txt EN\n",
      "53620.000.2016.1.1.txt EN\n",
      "42220.000.2013.1.1.txt DE\n",
      "51903.000.2015.1.1.txt EN\n",
      "42420.000.2008.1.1.txt DE\n",
      "41521.000.2017.1.1.txt DE\n",
      "41223.000.2017.1.1.txt DE\n",
      "53520.000.2007.1.1.txt EN\n",
      "43711.000.2011.1.1.txt DE\n",
      "51320.000.2015.1.1.txt EN\n",
      "43520.000.2011.1.1.txt DE\n",
      "41320.000.2013.1.1.txt DE\n",
      "51320.000.2010.1.1.txt EN\n",
      "51301.000.2010.1.1.txt EN\n",
      "51903.000.2010.1.1.txt EN\n",
      "53110.000.2016.1.1.txt EN\n",
      "43110.000.2015.1.1.txt DE\n",
      "53951.000.2007.1.1.txt EN\n",
      "53420.000.2007.1.1.txt EN\n",
      "41953.000.2013.1.1.txt DE\n",
      "51951.000.2017.1.1.txt EN\n",
      "42422.000.2013.1.1.txt DE\n",
      "42320.000.2013.1.1.txt DE\n",
      "42450.000.2017.1.1.txt DE\n",
      "42520.000.2008.1.1.txt DE\n",
      "43530.000.2011.1.1.txt DE\n",
      "53320.000.2011.1.1.txt EN\n",
      "43420.000.2015.1.1.txt DE\n",
      "41113.000.2009.1.1.txt DE\n",
      "51330.000.2010.1.1.txt EN\n",
      "51421.000.2015.1.1.txt EN\n",
      "43811.000.2011.1.1.txt DE\n",
      "53204.000.2016.1.1.txt EN\n",
      "42220.000.2017.1.1.txt DE\n",
      "42952.000.2008.1.1.txt DE\n",
      "41223.000.2013.1.1.txt DE\n",
      "41521.000.2013.1.1.txt DE\n",
      "53951.000.2016.1.1.txt EN\n",
      "53110.000.2007.1.1.txt EN\n",
      "41702.000.2009.1.1.txt DE\n",
      "51902.000.2010.1.1.txt EN\n",
      "41320.000.2009.1.1.txt DE\n",
      "51101.000.2017.1.1.txt EN\n",
      "43420.000.2007.1.1.txt DE\n",
      "43954.000.2015.1.1.txt DE\n",
      "51421.000.2017.1.1.txt EN\n",
      "51620.000.2010.1.1.txt EN\n",
      "42420.000.2017.1.1.txt DE\n",
      "41420.000.2017.1.1.txt DE\n",
      "53220.000.2016.1.1.txt EN\n",
      "43220.000.2015.1.1.txt DE\n",
      "53520.000.2011.1.1.txt EN\n",
      "53203.000.2011.1.1.txt EN\n",
      "42110.000.2017.1.1.txt DE\n",
      "43810.000.2011.1.1.txt DE\n",
      "53205.000.2016.1.1.txt EN\n",
      "41113.000.2013.1.1.txt DE\n",
      "43110.000.2007.1.1.txt DE\n",
      "42320.000.2008.1.1.txt DE\n",
      "41523.000.2017.1.1.txt DE\n",
      "53620.000.2011.1.1.txt EN\n",
      "43120.000.2015.1.1.txt DE\n",
      "42520.000.2013.1.1.txt DE\n",
      "43320.000.2011.1.1.txt DE\n",
      "51210.000.2015.1.1.txt EN\n",
      "51951.000.2015.1.1.txt EN\n",
      "41702.000.2013.1.1.txt DE\n",
      "41521.000.2009.1.1.txt DE\n",
      "41223.000.2009.1.1.txt DE\n"
     ]
    }
   ],
   "source": [
    "# this will be a dictionary of documents, for example manifestos, divided in sentences, which are represented as sentence embeddings\n",
    "collection = {}\n",
    "\n",
    "# Be cafeful here, you need to carefully map the language of the document so that I know which embeddings to use\n",
    "# for the moment I use the first number in the filename\n",
    "\n",
    "lang_map ={\"4\":\"DE\",\"5\":\"EN\"}\n",
    "\n",
    "for filename in [x for x in os.listdir(collection_path) if \".txt\" in x]:\n",
    "    lang = lang_map[filename[0]]\n",
    "    print (filename, lang)\n",
    "\n",
    "    # you open each file\n",
    "    # note encoding \n",
    "    content = codecs.open(collection_path+filename,\"r\",\"utf-8\").read()\n",
    "    \n",
    "    #remove breaklines\n",
    "    content = clean(content)\n",
    "    \n",
    "    # you split it in sentences\n",
    "    content = nltk.sent_tokenize(content)\n",
    "    \n",
    "    # you represent each sentence in each document as a word-embedding, which captures the meaning of the sentence\n",
    "    content = [[sent, text_embedding(sent,lang)] for sent in content if type(text_embedding(sent,lang))!= str]\n",
    "    collection[filename] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on how many sentences you want to retrieve\n",
    "max_sent = 50\n",
    "\n",
    "# filter on the cosine similarity\n",
    "\n",
    "threshold = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, the information retrieval part\n",
    "\n",
    "\n",
    "for filename,sentences in collection.items():\n",
    "    \n",
    "    lang = lang_map[filename[0]]\n",
    "    # compare the cosine similarity between the embedding of the query and each sentence embedding\n",
    "    ranking = [[sent, cosine_similarity(query_emb,sent_emb)[0][0]] for sent, sent_emb in sentences]\n",
    "    # you rank them, based on the similarity\n",
    "    ranking.sort(key=lambda x: x[1],reverse=True)\n",
    "    \n",
    "    # use this if you want to use max_sent\n",
    "    out = \" \"\n",
    "    for sent, score in ranking[:max_sent]:\n",
    "        out += sent+\" \"\n",
    "    \n",
    "    # use this if you want to use cosine similarity trheshold (comment max_sent part)\n",
    "    #out = \" \"\n",
    "    #for sent, score in ranking:\n",
    "    #    if score > threshold:\n",
    "    #        out += sent+\" \"   \n",
    "\n",
    "    # save selected sentences in files (so that you can use TopFish / Wordfish)\n",
    "    output = open(\"topic-output/filtered-\"+filename,\"w\")\n",
    "    output.write(lang+\"\\n\"+out)\n",
    "    output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered-51902.000.2017.1.1.txt 0.0\n",
      "filtered-51902.000.2015.1.1.txt 0.0336582162474567\n",
      "filtered-51301.000.2010.1.1.txt 0.048262262995620205\n",
      "filtered-51903.000.2015.1.1.txt 0.06110037600156924\n",
      "filtered-53202.000.2016.1.1.txt 0.14043733852124254\n",
      "filtered-53220.000.2011.1.1.txt 0.14800291974196017\n",
      "filtered-53520.000.2016.1.1.txt 0.15781466739111957\n",
      "filtered-51330.000.2010.1.1.txt 0.16058377329110307\n",
      "filtered-53620.000.2011.1.1.txt 0.17499753490746917\n",
      "filtered-53520.000.2011.1.1.txt 0.17689637006279205\n",
      "filtered-53620.000.2016.1.1.txt 0.1781059608907141\n",
      "filtered-51101.000.2017.1.1.txt 0.19128128622247212\n",
      "filtered-51320.000.2010.1.1.txt 0.19136855188626278\n",
      "filtered-53621.000.2016.1.1.txt 0.19158794860509307\n",
      "filtered-53951.000.2011.1.1.txt 0.19280910374109825\n",
      "filtered-51620.000.2015.1.1.txt 0.19286259372929912\n",
      "filtered-53620.000.2007.1.1.txt 0.19585305265628986\n",
      "filtered-51421.000.2010.1.1.txt 0.1971705271354065\n",
      "filtered-53951.000.2016.1.1.txt 0.197300943377592\n",
      "filtered-51951.000.2017.1.1.txt 0.19780163665413023\n",
      "filtered-51210.000.2010.1.1.txt 0.2044424416246077\n",
      "filtered-51903.000.2010.1.1.txt 0.20771739460052596\n",
      "filtered-51210.000.2015.1.1.txt 0.20865007316318993\n",
      "filtered-53203.000.2011.1.1.txt 0.21169519958913813\n",
      "filtered-51951.000.2015.1.1.txt 0.21572058164186358\n",
      "filtered-53110.000.2007.1.1.txt 0.21781252217915847\n",
      "filtered-53205.000.2016.1.1.txt 0.2189064562813034\n",
      "filtered-53110.000.2011.1.1.txt 0.21950988935366714\n",
      "filtered-51620.000.2010.1.1.txt 0.22100616445089305\n",
      "filtered-53520.000.2007.1.1.txt 0.22154140021925614\n",
      "filtered-53320.000.2011.1.1.txt 0.22494472809483304\n",
      "filtered-53321.000.2016.1.1.txt 0.2255782897829311\n",
      "filtered-53951.000.2007.1.1.txt 0.23480968252867893\n",
      "filtered-51421.000.2017.1.1.txt 0.23844756410115503\n",
      "filtered-51421.000.2015.1.1.txt 0.24171555856130428\n",
      "filtered-51101.000.2010.1.1.txt 0.2461188268379038\n",
      "filtered-51620.000.2017.1.1.txt 0.24679133173676523\n",
      "filtered-51320.000.2015.1.1.txt 0.2489642648502333\n",
      "filtered-53320.000.2016.1.1.txt 0.2541472477543197\n",
      "filtered-53204.000.2016.1.1.txt 0.2557597295824594\n",
      "filtered-53320.000.2007.1.1.txt 0.25984097421572905\n",
      "filtered-53220.000.2016.1.1.txt 0.26134184044151976\n",
      "filtered-53110.000.2016.1.1.txt 0.2653696379269603\n",
      "filtered-53420.000.2007.1.1.txt 0.26719530370337674\n",
      "filtered-51320.000.2017.1.1.txt 0.27017705093032773\n",
      "filtered-51401.000.2010.1.1.txt 0.2782598962349166\n",
      "filtered-53206.000.2016.1.1.txt 0.2853921996574517\n",
      "filtered-51902.000.2010.1.1.txt 0.38168522072892436\n",
      "filtered-42422.000.2013.1.1.txt 0.7679845647077955\n",
      "filtered-43520.000.2011.1.1.txt 0.7792827762471737\n",
      "filtered-43530.000.2011.1.1.txt 0.7796089495876466\n",
      "filtered-43520.000.2007.1.1.txt 0.7796305844371882\n",
      "filtered-42520.000.2008.1.1.txt 0.78638050648334\n",
      "filtered-43711.000.2011.1.1.txt 0.7892818269489705\n",
      "filtered-41953.000.2013.1.1.txt 0.7955682531489108\n",
      "filtered-43320.000.2015.1.1.txt 0.7963133426145222\n",
      "filtered-42951.000.2008.1.1.txt 0.8022369166957984\n",
      "filtered-42320.000.2017.1.1.txt 0.8056106509607067\n",
      "filtered-43420.000.2015.1.1.txt 0.8222322044872065\n",
      "filtered-42110.000.2008.1.1.txt 0.8227546747892418\n",
      "filtered-42420.000.2013.1.1.txt 0.8229003843783758\n",
      "filtered-43954.000.2015.1.1.txt 0.823839022061349\n",
      "filtered-43531.000.2007.1.1.txt 0.8296037316911128\n",
      "filtered-42450.000.2013.1.1.txt 0.8300365343964972\n",
      "filtered-43811.000.2011.1.1.txt 0.833989391741442\n",
      "filtered-42421.000.2008.1.1.txt 0.8397696912994155\n",
      "filtered-42220.000.2013.1.1.txt 0.8469391762972988\n",
      "filtered-42420.000.2008.1.1.txt 0.8519852139880635\n",
      "filtered-41521.000.2017.1.1.txt 0.8524975810950113\n",
      "filtered-43420.000.2011.1.1.txt 0.8536605118096495\n",
      "filtered-42110.000.2013.1.1.txt 0.8587984914758141\n",
      "filtered-43520.000.2015.1.1.txt 0.8592929173834698\n",
      "filtered-42520.000.2017.1.1.txt 0.8621899606380823\n",
      "filtered-43320.000.2011.1.1.txt 0.8688403187684385\n",
      "filtered-43320.000.2007.1.1.txt 0.8709050649558141\n",
      "filtered-43530.000.2007.1.1.txt 0.8734956306616308\n",
      "filtered-41523.000.2017.1.1.txt 0.8751237960708651\n",
      "filtered-41702.000.2013.1.1.txt 0.8787060577076476\n",
      "filtered-41420.000.2017.1.1.txt 0.880607506940436\n",
      "filtered-43110.000.2007.1.1.txt 0.8813973570798823\n",
      "filtered-43810.000.2007.1.1.txt 0.8826218131679945\n",
      "filtered-43420.000.2007.1.1.txt 0.885318016732835\n",
      "filtered-41223.000.2013.1.1.txt 0.8886890511288535\n",
      "filtered-42220.000.2017.1.1.txt 0.8905696614397232\n",
      "filtered-43810.000.2011.1.1.txt 0.8911319225469327\n",
      "filtered-42952.000.2008.1.1.txt 0.8923591065066747\n",
      "filtered-42320.000.2008.1.1.txt 0.8958272111490104\n",
      "filtered-41113.000.2009.1.1.txt 0.8982332668064049\n",
      "filtered-42320.000.2013.1.1.txt 0.8984996306060359\n",
      "filtered-41320.000.2013.1.1.txt 0.9025275231214062\n",
      "filtered-42110.000.2017.1.1.txt 0.9054240642725639\n",
      "filtered-43110.000.2011.1.1.txt 0.9145596875562138\n",
      "filtered-41521.000.2013.1.1.txt 0.9189492539924901\n",
      "filtered-41950.000.2013.1.1.txt 0.9207326295361957\n",
      "filtered-43530.000.2015.1.1.txt 0.9212746757333822\n",
      "filtered-41320.000.2017.1.1.txt 0.922231187792406\n",
      "filtered-41113.000.2017.1.1.txt 0.9244409484458623\n",
      "filtered-42420.000.2017.1.1.txt 0.924767486151513\n",
      "filtered-43120.000.2015.1.1.txt 0.9268009199161009\n",
      "filtered-42450.000.2017.1.1.txt 0.9330406829298078\n",
      "filtered-43220.000.2015.1.1.txt 0.9394016076904993\n",
      "filtered-41320.000.2009.1.1.txt 0.94230920428969\n",
      "filtered-41521.000.2009.1.1.txt 0.9483555788343471\n",
      "filtered-41950.000.2009.1.1.txt 0.9495061084146057\n",
      "filtered-43810.000.2015.1.1.txt 0.9528600729484712\n",
      "filtered-41420.000.2013.1.1.txt 0.9567125494642414\n",
      "filtered-43110.000.2015.1.1.txt 0.9577672461084514\n",
      "filtered-41223.000.2009.1.1.txt 0.9588628336091574\n",
      "filtered-41953.000.2017.1.1.txt 0.9662676665020201\n",
      "filtered-42520.000.2013.1.1.txt 0.9728054045965371\n",
      "filtered-41113.000.2013.1.1.txt 0.9733692261962076\n",
      "filtered-41223.000.2017.1.1.txt 0.9737174003128499\n",
      "filtered-41702.000.2009.1.1.txt 0.9937856388749098\n",
      "filtered-41420.000.2009.1.1.txt 1.0\n"
     ]
    }
   ],
   "source": [
    "# add yours\n",
    "\n",
    "import shutil\n",
    "shutil.rmtree('topic-output/.ipynb_checkpoints/')\n",
    "\n",
    "topfish_path = \"/Users/federiconanni/topfish/scaler.py\"\n",
    "topfish_emb_path = \"/Users/federiconanni/topfish/wiki.big-five.mapped.vec\"\n",
    "out_file = \"topic-scaling.txt\"\n",
    "subprocess.call(\"python \"+topfish_path+\" topic-output/ \"+topfish_emb_path+\" \"+out_file, shell=True)\n",
    "\n",
    "scaling = open(out_file,\"r\").read().strip().split(\"\\n\")\n",
    "scaling = [x.split() for x in scaling]\n",
    "scaling.sort(key=lambda x: x[1])\n",
    "for el in scaling:\n",
    "    print (\" \".join(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
