{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, nltk, string, os, gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# this represent any text as a single \"doc-embedding\" we use it both for the query and the sentences\n",
    "# input should be a string\n",
    "def text_embedding(text,lang):\n",
    "    \n",
    "    lang = lang.lower()\n",
    "    \n",
    "    #you should check in the embeddings you use if the words have been lowercased or not. \n",
    "    #try ask the embedding for \"barack\" and for \"Barack\"\n",
    "    # if the Barack works, then comment the following line\n",
    "    text = text.lower()\n",
    "    \n",
    "    # we tokenize the text in single words\n",
    "    text = nltk.tokenize.WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    # we remove numbers and punctuation\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    \n",
    "    doc_embed = []\n",
    "    \n",
    "    # for each word we get the embedding and we append it to a list\n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = emb_model[lang+\"__\"+word]\n",
    "                doc_embed.append(embed_word)\n",
    "            except KeyError:\n",
    "#                print (\"not found:\", word)\n",
    "                continue\n",
    "    # we average the embeddings of all the words, getting an overall doc embedding\n",
    "    if len(doc_embed)>0:\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*doc_embed)]\n",
    "\n",
    "        avg = np.array(avg).reshape(1, -1)\n",
    "\n",
    "        # the output is a doc-embedding\n",
    "        return avg\n",
    "    else:\n",
    "        return \"Empty\"\n",
    "    \n",
    "def clean(text):\n",
    "    text = text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = gensim.models.KeyedVectors.load_word2vec_format('../../../TextScaling/topfish/edited.wiki.big-five.mapped.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"elite politiker establishment herrschend\"\n",
    "\n",
    "lang = \"DE\"\n",
    "\n",
    "query_emb = text_embedding(query, lang)\n",
    "\n",
    "# add the path to the folder where you have your manifestos as text documents\n",
    "# collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/deu2017/\"\n",
    "\n",
    "# only Germany for less output \n",
    "#collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/germany17/\"\n",
    "#collection_path = \"../resources/deu2017/\"\n",
    "\n",
    "collection_path = \"/Users/federiconanni/Dropbox/University/research/sparserhetoric/polidoc_bigfive_longitude/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be a dictionary of documents, for example manifestos, divided in sentences, which are represented as sentence embeddings\n",
    "collection = {}\n",
    "\n",
    "# Be cafeful here, you need to carefully map the language of the document so that I know which embeddings to use\n",
    "# for the moment I use the first number in the filename\n",
    "\n",
    "lang_map ={\"4\":\"DE\",\"5\":\"EN\"}\n",
    "\n",
    "for filename in [x for x in os.listdir(collection_path) if \".txt\" in x]:\n",
    "    lang = lang_map[filename[0]]\n",
    "    print (filename, lang)\n",
    "\n",
    "    # you open each file\n",
    "    # note encoding \n",
    "    content = codecs.open(collection_path+filename,\"r\",\"utf-8\").read()\n",
    "    \n",
    "    #remove breaklines\n",
    "    content = clean(content)\n",
    "    \n",
    "    # you split it in sentences\n",
    "    content = nltk.sent_tokenize(content)\n",
    "    \n",
    "    # you represent each sentence in each document as a word-embedding, which captures the meaning of the sentence\n",
    "    content = [[sent, text_embedding(sent,lang)] for sent in content if type(text_embedding(sent,lang))!= str]\n",
    "    collection[filename] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on how many sentences you want to retrieve\n",
    "max_sent = 50\n",
    "\n",
    "# filter on the cosine similarity\n",
    "\n",
    "threshold = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, the information retrieval part\n",
    "\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('topic-output/')\n",
    "except Exception as e:\n",
    "    print (e)\n",
    "\n",
    "    \n",
    "import os\n",
    "if not os.path.exists('topic-output/'):\n",
    "    os.makedirs('topic-output/')\n",
    "\n",
    "    \n",
    "for filename,sentences in collection.items():\n",
    "    \n",
    "    lang = lang_map[filename[0]]\n",
    "    # compare the cosine similarity between the embedding of the query and each sentence embedding\n",
    "    ranking = [[sent, cosine_similarity(query_emb,sent_emb)[0][0]] for sent, sent_emb in sentences]\n",
    "    # you rank them, based on the similarity\n",
    "    ranking.sort(key=lambda x: x[1],reverse=True)\n",
    "    \n",
    "    # use this if you want to use max_sent\n",
    "    out = \" \"\n",
    "    for sent, score in ranking[:max_sent]:\n",
    "        out += sent+\" \"\n",
    "    \n",
    "    # use this if you want to use cosine similarity trheshold (comment max_sent part)\n",
    "    #out = \" \"\n",
    "    #for sent, score in ranking:\n",
    "    #    if score > threshold:\n",
    "    #        out += sent+\" \"   \n",
    "\n",
    "    # save selected sentences in files (so that you can use TopFish / Wordfish)\n",
    "    output = open(\"topic-output/filtered-\"+filename,\"w\")\n",
    "    output.write(lang+\"\\n\"+out)\n",
    "    output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add yours\n",
    "    \n",
    "topfish_emb_path = \"/Users/federiconanni/topfish/wiki.big-five.mapped.vec\"\n",
    "topfish_path = \"/Users/federiconanni/topfish/scaler.py\"\n",
    "out_file = \"topic-scaling.txt\"\n",
    "\n",
    "\n",
    "subprocess.call(\"python \"+topfish_path+\" topic-output/ \"+topfish_emb_path+\" \"+out_file, shell=True)\n",
    "\n",
    "scaling = open(out_file,\"r\").read().strip().split(\"\\n\")\n",
    "scaling = [x.split() for x in scaling]\n",
    "scaling.sort(key=lambda x: x[1])\n",
    "for el in scaling:\n",
    "    print (\" \".join(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
