{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, nltk, string, os, gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# this represent any text as a single \"doc-embedding\" we use it both for the query and the sentences\n",
    "# input should be a string\n",
    "def text_embedding(text,lang):\n",
    "    \n",
    "    lang = lang.lower()\n",
    "    \n",
    "    #you should check in the embeddings you use if the words have been lowercased or not. \n",
    "    #try ask the embedding for \"barack\" and for \"Barack\"\n",
    "    # if the Barack works, then comment the following line\n",
    "    text = text.lower()\n",
    "    \n",
    "    # we tokenize the text in single words\n",
    "    text = nltk.tokenize.WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    # we remove numbers and punctuation\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    \n",
    "    doc_embed = []\n",
    "    \n",
    "    # for each word we get the embedding and we append it to a list\n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = emb_model[lang+\"__\"+word]\n",
    "                doc_embed.append(embed_word)\n",
    "            except KeyError:\n",
    "#                print (\"not found:\", word)\n",
    "                continue\n",
    "    # we average the embeddings of all the words, getting an overall doc embedding\n",
    "    if len(doc_embed)>0:\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*doc_embed)]\n",
    "\n",
    "        avg = np.array(avg).reshape(1, -1)\n",
    "\n",
    "        # the output is a doc-embedding\n",
    "        return avg\n",
    "    else:\n",
    "        return \"Empty\"\n",
    "    \n",
    "def clean(text):\n",
    "    text = text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_model = gensim.models.KeyedVectors.load_word2vec_format('../../../TextScaling/topfish/edited.wiki.big-five.mapped.vec', binary=False)\n",
    "emb_model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Dr. J/Desktop/edited.wiki.big-five.mapped.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query = \"elite politiker establishment herrschend\"\n",
    "\n",
    "# query = \"people elite sovereign\"\n",
    "\n",
    "# query = \"volk elite souverän\"\n",
    "\n",
    "query = \"regierung parlament repräsentation\"\n",
    "\n",
    "# query = \"parliament government representation\"\n",
    "\n",
    "lang = \"DE\"\n",
    "\n",
    "#lang = \"EN\"\n",
    "\n",
    "query_emb = text_embedding(query, lang)\n",
    "\n",
    "# add the path to the folder where you have your manifestos as text documents\n",
    "# collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/deu2017/\"\n",
    "\n",
    "# only Germany for less output \n",
    "#collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/germany17/\"\n",
    "#collection_path = \"../resources/deu2017/\"\n",
    "\n",
    "#collection_path = \"/Users/federiconanni/Dropbox/University/research/sparserhetoric/polidoc_bigfive_longitude/\"\n",
    "\n",
    "collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/polidoc_bigfive_longitude/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41113.000.2009.1.1.txt DE\n",
      "41113.000.2013.1.1.txt DE\n",
      "41113.000.2017.1.1.txt DE\n",
      "41223.000.2009.1.1.txt DE\n",
      "41223.000.2013.1.1.txt DE\n",
      "41223.000.2017.1.1.txt DE\n",
      "41320.000.2009.1.1.txt DE\n",
      "41320.000.2013.1.1.txt DE\n",
      "41320.000.2017.1.1.txt DE\n",
      "41420.000.2009.1.1.txt DE\n",
      "41420.000.2013.1.1.txt DE\n",
      "41420.000.2017.1.1.txt DE\n",
      "41521.000.2009.1.1.txt DE\n",
      "41521.000.2013.1.1.txt DE\n",
      "41521.000.2017.1.1.txt DE\n",
      "41523.000.2017.1.1.txt DE\n",
      "41702.000.2009.1.1.txt DE\n",
      "41702.000.2013.1.1.txt DE\n",
      "41950.000.2009.1.1.txt DE\n",
      "41950.000.2013.1.1.txt DE\n",
      "41953.000.2013.1.1.txt DE\n",
      "41953.000.2017.1.1.txt DE\n",
      "42110.000.2008.1.1.txt DE\n",
      "42110.000.2013.1.1.txt DE\n",
      "42110.000.2017.1.1.txt DE\n",
      "42220.000.2013.1.1.txt DE\n",
      "42220.000.2017.1.1.txt DE\n",
      "42320.000.2008.1.1.txt DE\n",
      "42320.000.2013.1.1.txt DE\n",
      "42320.000.2017.1.1.txt DE\n",
      "42420.000.2008.1.1.txt DE\n",
      "42420.000.2013.1.1.txt DE\n",
      "42420.000.2017.1.1.txt DE\n",
      "42421.000.2008.1.1.txt DE\n",
      "42422.000.2013.1.1.txt DE\n",
      "42450.000.2013.1.1.txt DE\n",
      "42450.000.2017.1.1.txt DE\n",
      "42520.000.2008.1.1.txt DE\n",
      "42520.000.2013.1.1.txt DE\n",
      "42520.000.2017.1.1.txt DE\n",
      "42951.000.2008.1.1.txt DE\n",
      "42952.000.2008.1.1.txt DE\n",
      "43110.000.2007.1.1.txt DE\n",
      "43110.000.2011.1.1.txt DE\n",
      "43110.000.2015.1.1.txt DE\n",
      "43120.000.2015.1.1.txt DE\n",
      "43220.000.2015.1.1.txt DE\n",
      "43320.000.2007.1.1.txt DE\n",
      "43320.000.2011.1.1.txt DE\n",
      "43320.000.2015.1.1.txt DE\n",
      "43420.000.2007.1.1.txt DE\n",
      "43420.000.2011.1.1.txt DE\n",
      "43420.000.2015.1.1.txt DE\n",
      "43520.000.2007.1.1.txt DE\n",
      "43520.000.2011.1.1.txt DE\n",
      "43520.000.2015.1.1.txt DE\n",
      "43530.000.2007.1.1.txt DE\n",
      "43530.000.2011.1.1.txt DE\n",
      "43530.000.2015.1.1.txt DE\n",
      "43531.000.2007.1.1.txt DE\n",
      "43711.000.2011.1.1.txt DE\n",
      "43810.000.2007.1.1.txt DE\n",
      "43810.000.2011.1.1.txt DE\n",
      "43810.000.2015.1.1.txt DE\n",
      "43811.000.2011.1.1.txt DE\n",
      "43954.000.2015.1.1.txt DE\n",
      "51101.000.2010.1.1.txt EN\n",
      "51101.000.2017.1.1.txt EN\n",
      "51210.000.2010.1.1.txt EN\n",
      "51210.000.2015.1.1.txt EN\n",
      "51301.000.2010.1.1.txt EN\n",
      "51320.000.2010.1.1.txt EN\n",
      "51320.000.2015.1.1.txt EN\n",
      "51320.000.2017.1.1.txt EN\n",
      "51330.000.2010.1.1.txt EN\n",
      "51401.000.2010.1.1.txt EN\n",
      "51421.000.2010.1.1.txt EN\n",
      "51421.000.2015.1.1.txt EN\n",
      "51421.000.2017.1.1.txt EN\n",
      "51620.000.2010.1.1.txt EN\n",
      "51620.000.2015.1.1.txt EN\n",
      "51620.000.2017.1.1.txt EN\n",
      "51902.000.2010.1.1.txt EN\n",
      "51902.000.2015.1.1.txt EN\n",
      "51902.000.2017.1.1.txt EN\n",
      "51903.000.2010.1.1.txt EN\n",
      "51903.000.2015.1.1.txt EN\n",
      "51951.000.2015.1.1.txt EN\n",
      "51951.000.2017.1.1.txt EN\n",
      "53110.000.2007.1.1.txt EN\n",
      "53110.000.2011.1.1.txt EN\n",
      "53110.000.2016.1.1.txt EN\n",
      "53202.000.2016.1.1.txt EN\n",
      "53203.000.2011.1.1.txt EN\n",
      "53204.000.2016.1.1.txt EN\n",
      "53205.000.2016.1.1.txt EN\n",
      "53206.000.2016.1.1.txt EN\n",
      "53220.000.2011.1.1.txt EN\n",
      "53220.000.2016.1.1.txt EN\n",
      "53320.000.2007.1.1.txt EN\n",
      "53320.000.2011.1.1.txt EN\n",
      "53320.000.2016.1.1.txt EN\n",
      "53321.000.2016.1.1.txt EN\n",
      "53420.000.2007.1.1.txt EN\n",
      "53520.000.2007.1.1.txt EN\n",
      "53520.000.2011.1.1.txt EN\n",
      "53520.000.2016.1.1.txt EN\n",
      "53620.000.2007.1.1.txt EN\n",
      "53620.000.2011.1.1.txt EN\n",
      "53620.000.2016.1.1.txt EN\n",
      "53621.000.2016.1.1.txt EN\n",
      "53951.000.2007.1.1.txt EN\n",
      "53951.000.2011.1.1.txt EN\n",
      "53951.000.2016.1.1.txt EN\n"
     ]
    }
   ],
   "source": [
    "# this will be a dictionary of documents, for example manifestos, divided in sentences, which are represented as sentence embeddings\n",
    "collection = {}\n",
    "\n",
    "# Be cafeful here, you need to carefully map the language of the document so that I know which embeddings to use\n",
    "# for the moment I use the first number in the filename\n",
    "\n",
    "lang_map ={\"4\":\"DE\",\"5\":\"EN\"}\n",
    "\n",
    "for filename in [x for x in os.listdir(collection_path) if \".txt\" in x]:\n",
    "    lang = lang_map[filename[0]]\n",
    "    print (filename, lang)\n",
    "\n",
    "    # you open each file\n",
    "    # note encoding \n",
    "    content = codecs.open(collection_path+filename,\"r\",\"utf-8\").read()\n",
    "    \n",
    "    #remove breaklines\n",
    "    content = clean(content)\n",
    "    \n",
    "    # you split it in sentences\n",
    "    content = nltk.sent_tokenize(content)\n",
    "    \n",
    "    # you represent each sentence in each document as a word-embedding, which captures the meaning of the sentence\n",
    "    content = [[sent, text_embedding(sent,lang)] for sent in content if type(text_embedding(sent,lang))!= str]\n",
    "    collection[filename] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on how many sentences you want to retrieve\n",
    "max_sent = 40\n",
    "\n",
    "# filter on the cosine similarity\n",
    "\n",
    "threshold = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, the information retrieval part\n",
    "\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('topic-output/')\n",
    "except Exception as e:\n",
    "    print (e)\n",
    "\n",
    "    \n",
    "import os\n",
    "if not os.path.exists('topic-output/'):\n",
    "    os.makedirs('topic-output/')\n",
    "\n",
    "    \n",
    "for filename,sentences in collection.items():\n",
    "    \n",
    "    lang = lang_map[filename[0]]\n",
    "    # compare the cosine similarity between the embedding of the query and each sentence embedding\n",
    "    ranking = [[sent, cosine_similarity(query_emb,sent_emb)[0][0]] for sent, sent_emb in sentences]\n",
    "    # you rank them, based on the similarity\n",
    "    ranking.sort(key=lambda x: x[1],reverse=True)\n",
    "    \n",
    "    # use this if you want to use max_sent\n",
    "    out = \" \"\n",
    "    for sent, score in ranking[:max_sent]:\n",
    "        out += sent+\" \"\n",
    "    \n",
    "    # use this if you want to use cosine similarity trheshold (comment max_sent part)\n",
    "    #out = \" \"\n",
    "    #for sent, score in ranking:\n",
    "    #    if score > threshold:\n",
    "    #        out += sent+\" \"   \n",
    "\n",
    "    # save selected sentences in files (so that you can use TopFish / Wordfish)\n",
    "    #output = open(\"topic-output/filtered-\"+filename,\"w\")\n",
    "    output = codecs.open(\"C:/Users/Dr. J/Desktop/topic-output/filtered-\"+filename,\"w\",\"UTF-8\")\n",
    "    output.write(lang+\"\\n\"+out)\n",
    "    output.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add yours\n",
    "    \n",
    "topfish_emb_path = \"/Users/federiconanni/topfish/wiki.big-five.mapped.vec\"\n",
    "topfish_path = \"/Users/federiconanni/topfish/scaler.py\"\n",
    "out_file = \"topic-scaling.txt\"\n",
    "\n",
    "\n",
    "subprocess.call(\"python \"+topfish_path+\" topic-output/ \"+topfish_emb_path+\" \"+out_file, shell=True)\n",
    "\n",
    "scaling = open(out_file,\"r\").read().strip().split(\"\\n\")\n",
    "scaling = [x.split() for x in scaling]\n",
    "scaling.sort(key=lambda x: x[1])\n",
    "for el in scaling:\n",
    "    print (\" \".join(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-590dfbf8a82d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msts\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msimple_sts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mM:\\topfish-master\\nlp.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_shaper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# alternative JB machine \n",
    "\n",
    "import os\n",
    "os.chdir('M:\\\\topfish-master')\n",
    "\n",
    "from embeddings import text_embeddings\n",
    "import nlp\n",
    "from helpers import io_helper\n",
    "from sts import simple_sts \n",
    "from sys import stdin\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "supported_lang_strings = {\"en\" : \"english\", \"fr\" : \"french\", \"de\" : \"german\", \"es\" : \"spanish\", \"it\" : \"italian\"}\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Performs text scaling (assigns a score to each text on a linear scale).')\n",
    "parser.add_argument('-d', '--datadir', default='C:\\\\Users\\\\Dr. J\\\\Desktop\\\\topic-output', help='A path to the directory containing the input text files for scaling (one score will be assigned per file).')\n",
    "parser.add_argument('-e','--embs', default='M:\\\\topfish-master\\\\embeddingspaces\\\\wiki.big-five.mapped.vec', help='A path to the file containing pre-trained word embeddings')\n",
    "parser.add_argument('-o', '--output', default='C:\\\\Users\\\\Dr. J\\\\Desktop\\\\output2',  help='Path for scaling results.')\n",
    "parser.add_argument('-s', '--stopwords', default='M:\\\\topfish-master\\\\stopwords\\\\bigfive.txt', help='Path for stopwords')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "files = io_helper.load_all_files(args.datadir)\n",
    "filenames = [x[0] for x in files]\n",
    "texts = [x[1] for x in files]\n",
    "filenames\n",
    "\n",
    "languages = [x.split(\"\\n\", 1)[0].strip().lower() for x in texts]\n",
    "languages\n",
    "langs = [(l if l in supported_lang_strings.values() else supported_lang_strings[l]) for l in languages]\n",
    "\n",
    "stopwords = io_helper.load_lines(args.stopwords)\n",
    "\n",
    "predictions_serialization_path = args.output\n",
    "\n",
    "embeddings = text_embeddings.Embeddings()\n",
    "embeddings.load_embeddings(args.embs, limit = 1000000, language = 'default', print_loading = True, skip_first_line = True)\n",
    "\n",
    "nlp.scale_efficient(filenames, texts, langs, embeddings, predictions_serialization_path, stopwords)\n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \" Scaling completed.\", flush = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
