{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import codecs, nltk, string, os, gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# this represent any text as a single \"doc-embedding\" we use it both for the query and the sentences\n",
    "# input should be a string\n",
    "def text_embedding(text):\n",
    "    \n",
    "    #you should check in the embeddings you use if the words have been lowercased or not. \n",
    "    #try ask the embedding for \"barack\" and for \"Barack\"\n",
    "    # if the Barack works, then comment the following line\n",
    "    text = text.lower()\n",
    "    \n",
    "    # we tokenize the text in single words\n",
    "    text = nltk.tokenize.WordPunctTokenizer().tokenize(text)\n",
    "    \n",
    "    # we remove numbers and punctuation\n",
    "    text = [token for token in text if token not in exclude and token.isalpha()]\n",
    "    \n",
    "    \n",
    "    doc_embed = []\n",
    "    \n",
    "    # for each word we get the embedding and we append it to a list\n",
    "    for word in text:\n",
    "            try:\n",
    "                embed_word = emb_model[word]\n",
    "                doc_embed.append(embed_word)\n",
    "            except KeyError:\n",
    "                continue\n",
    "    # we average the embeddings of all the words, getting an overall doc embedding\n",
    "    if len(doc_embed)>0:\n",
    "        avg = [float(sum(col))/len(col) for col in zip(*doc_embed)]\n",
    "\n",
    "        avg = np.array(avg).reshape(1, -1)\n",
    "\n",
    "        # the output is a doc-embedding\n",
    "        return avg\n",
    "    else:\n",
    "        return \"Empty\"\n",
    "    \n",
    "def clean(text):\n",
    "    text = text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import here your word-embeddings - put the path to the file (if it's .bin change the binary to True)\n",
    "#emb_model = gensim.models.KeyedVectors.load_word2vec_format('../../resources/small-embeddings.txt', binary=False)\n",
    "# german wikipedia from https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md\n",
    "emb_model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Dr. J/Desktop/wiki.de.vec', binary=False)\n",
    "\n",
    "# switch for in-domain \n",
    "#emb_model = gensim.models.KeyedVectors.load_word2vec_format('in-domain-embeddings.txt', binary=False)\n",
    "#emb_model = gensim.models.KeyedVectors.load_word2vec_format('C:/QTA/topic-detect/in-domain-embeddings.txt', binary=False)\n",
    "\n",
    "# big five - not working \n",
    "#emb_model = gensim.models.KeyedVectors.load_word2vec_format('U:/topfish-master/embeddingspaces/wiki.big-five.mapped.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this can be a list of words on the same fine-grained topic, like \"people\", \"elites\"\n",
    "# add more words after a space to make it more precise\n",
    "# query = \"volk bürger\"\n",
    "\n",
    "# focus on \"key key\" terms based on dictionary, let the embeddings find more \n",
    "#query = \"volk\"\n",
    "#query = \"bürger einbürgerung steuerzahler gemeinschaft\"\n",
    "#query = \"elite politiker establishment herrschend\"\n",
    "#query = \"korruption täuschung betrügen verrat schämen skandal wahrheit unehrlich lüge\"\n",
    "#query = \"verantwortung glaubwürdigkeit\"\n",
    "#query = \"souverän neutral\"\n",
    "#query = \"demokratisch referendum volksabstimmung volksinitiative\"\n",
    "#query = \"konsens kompromiss\"\n",
    "#query = \"repräsentation parlament regierung\"\n",
    "#query = \"populisten populismus demagogisch demagogen\"\n",
    "\n",
    "# query = \"volk elite souverän\"\n",
    "# query = \"volk elite politiker establishment herrschend souverän\"\n",
    "# query = \"konsens kompromiss repräsentation populisten populismus\"\n",
    "\n",
    "# populism at its best?\n",
    "# query = \"volk elite souverän\"\n",
    "\n",
    "# efficient flip-side query? \n",
    "# query = \"konsens repräsentation populismus\"\n",
    "# works not so well \n",
    "# query = \"konsens populismus\" - nope\n",
    "# query = \"populismus populisten parlament regierung repräsentation\" - turns out to measure the same thing!\n",
    "\n",
    "# all in query \n",
    "query = \"volk elite souverän parlament regierung repräsentation\"\n",
    "\n",
    "query_emb = text_embedding(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the path to the folder where you have your manifestos as text documents\n",
    "\n",
    "# collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/deu2017/\"\n",
    "# collection_path = \"../../resources/deu2017/\"\n",
    "\n",
    "# only Germany for less output \n",
    "# collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/germany17/\"\n",
    "\n",
    "# all German manifestos from polidoc as of 180911\n",
    "# collection_path = \"U:/QTA PA/polidoc_allgerman/\"\n",
    "\n",
    "# German texts last three elections for Germany, Austria and Switzerland - 63 manifestos from polidoc.net\n",
    "collection_path = \"C:/Users/Dr. J/Dropbox/sparserhetoric/polidoc_allgerman/\"\n",
    "\n",
    "# full sample eight countries all manifestos on polidoc (will be updated)\n",
    "# collection_path = \"U:/QTA PA/polidoc big five 180910/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41113.000.2009.1.1.txt\n",
      "41113.000.2013.1.1.txt\n",
      "41113.000.2017.1.1.txt\n",
      "41223.000.2009.1.1.txt\n",
      "41223.000.2013.1.1.txt\n",
      "41223.000.2017.1.1.txt\n",
      "41320.000.2009.1.1.txt\n",
      "41320.000.2013.1.1.txt\n",
      "41320.000.2017.1.1.txt\n",
      "41420.000.2009.1.1.txt\n",
      "41420.000.2013.1.1.txt\n",
      "41420.000.2017.1.1.txt\n",
      "41521.000.2009.1.1.txt\n",
      "41521.000.2013.1.1.txt\n",
      "41521.000.2017.1.1.txt\n",
      "41523.000.2017.1.1.txt\n",
      "41702.000.2009.1.1.txt\n",
      "41702.000.2013.1.1.txt\n",
      "41950.000.2009.1.1.txt\n",
      "41950.000.2013.1.1.txt\n",
      "41953.000.2013.1.1.txt\n",
      "41953.000.2017.1.1.txt\n",
      "42110.000.2008.1.1.txt\n",
      "42110.000.2013.1.1.txt\n",
      "42110.000.2017.1.1.txt\n",
      "42220.000.2013.1.1.txt\n",
      "42220.000.2017.1.1.txt\n",
      "42320.000.2008.1.1.txt\n",
      "42320.000.2013.1.1.txt\n",
      "42320.000.2017.1.1.txt\n",
      "42420.000.2008.1.1.txt\n",
      "42420.000.2013.1.1.txt\n",
      "42420.000.2017.1.1.txt\n",
      "42421.000.2008.1.1.txt\n",
      "42422.000.2013.1.1.txt\n",
      "42450.000.2013.1.1.txt\n",
      "42450.000.2017.1.1.txt\n",
      "42520.000.2008.1.1.txt\n",
      "42520.000.2013.1.1.txt\n",
      "42520.000.2017.1.1.txt\n",
      "42951.000.2008.1.1.txt\n",
      "42952.000.2008.1.1.txt\n",
      "43110.000.2007.1.1.txt\n",
      "43110.000.2011.1.1.txt\n",
      "43110.000.2015.1.1.txt\n",
      "43120.000.2015.1.1.txt\n",
      "43220.000.2015.1.1.txt\n",
      "43320.000.2007.1.1.txt\n",
      "43320.000.2011.1.1.txt\n",
      "43320.000.2015.1.1.txt\n",
      "43420.000.2007.1.1.txt\n",
      "43420.000.2011.1.1.txt\n",
      "43420.000.2015.1.1.txt\n",
      "43520.000.2007.1.1.txt\n",
      "43520.000.2011.1.1.txt\n",
      "43520.000.2015.1.1.txt\n",
      "43530.000.2007.1.1.txt\n",
      "43530.000.2011.1.1.txt\n",
      "43530.000.2015.1.1.txt\n",
      "43531.000.2007.1.1.txt\n",
      "43711.000.2011.1.1.txt\n",
      "43810.000.2007.1.1.txt\n",
      "43810.000.2011.1.1.txt\n",
      "43810.000.2015.1.1.txt\n",
      "43811.000.2011.1.1.txt\n",
      "43953.000.2007.1.1.txt\n",
      "43954.000.2015.1.1.txt\n"
     ]
    }
   ],
   "source": [
    "# this will be a dictionary of documents, for example manifestos, divided in sentences, which are represented as sentence embeddings\n",
    "collection = {}\n",
    "\n",
    "# you loop over the folder\n",
    "for filename in [x for x in os.listdir(collection_path) if \".txt\" in x]:\n",
    "    print (filename)\n",
    "    # you open each file\n",
    "    # note encoding \n",
    "#    content = codecs.open(collection_path+filename,\"r\",\"utf-8\").read()\n",
    "    content = codecs.open(collection_path+filename,\"r\",\"utf-8-sig\").read()\n",
    "    \n",
    "    #remove breaklines\n",
    "    content = clean(content)\n",
    "    \n",
    "    # you split it in sentences\n",
    "    content = nltk.sent_tokenize(content)\n",
    "    \n",
    "    # you represent each sentence in each document as a word-embedding, which captures the meaning of the sentence\n",
    "    content = [[sent, text_embedding(sent)] for sent in content if type(text_embedding(sent))!= str]\n",
    "    collection[filename] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on how many sentences you want to retrieve\n",
    "#max_sent = 50\n",
    "max_sent = 20\n",
    "\n",
    "# filter on the cosine similarity\n",
    "\n",
    "threshold = 0.55\n",
    "#threshold = 0.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, the information retrieval part\n",
    "\n",
    "\n",
    "for filename,sentences in collection.items():\n",
    "    \n",
    "    # compare the cosine similarity between the embedding of the query and each sentence embedding\n",
    "    ranking = [[sent, cosine_similarity(query_emb,sent_emb)[0][0]] for sent, sent_emb in sentences]\n",
    "    # you rank them, based on the similarity\n",
    "    ranking.sort(key=lambda x: x[1],reverse=True)\n",
    "    \n",
    "    # use this if you want to use max_sent\n",
    "    out = \" \"\n",
    "    for sent, score in ranking[:max_sent]:\n",
    "        out += sent+\" \"\n",
    "    \n",
    "    # use this if you want to use cosine similarity trheshold (comment max_sent part)\n",
    "    #out = \" \"\n",
    "    #for sent, score in ranking:\n",
    "    #    if score > threshold:\n",
    "    #        out += sent+\" \"   \n",
    "\n",
    "    # save selected sentences in files (so that you can use TopFish / Wordfish)\n",
    "    #output = open(\"topic-output/filtered-\"+filename,\"w\")\n",
    "    output = codecs.open(\"C:/Users/Dr. J/Desktop/topic-output/filtered-\"+filename,\"w\",\"UTF-8\")\n",
    "    output.write(\"DE\\n\"+out)\n",
    "    output.close()\n",
    "    \n",
    "    # would be nice: write to \"utf-8\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add yours\n",
    "#topfish_path = \"/Users/federiconanni/topfish/scaler.py\"\n",
    "topfish_path = \"M:/topfish-master/scaler_j.py\"\n",
    "#topfish_emb_path = \"/Users/federiconanni/topfish/wiki.big-five.mapped.vec\"\n",
    "topfish_emb_path = \"M:/topfish-master/embeddingspaces/wiki.big-five.mapped.vec\"\n",
    "out_file = \"C:/Users/Dr. J/Desktop/output2\"\n",
    "subprocess.call(\"python \"+topfish_path+\" topic-output/ \"+topfish_emb_path+\" \"+out_file, shell=True)\n",
    "# not running, I do it separately (scaler_j.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = \"C:/Users/Dr. J/Desktop/output2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered-43120.000.2015.1.1.txt 0.0\n",
      "filtered-42320.000.2013.1.1.txt 0.11117555396460413\n",
      "filtered-43520.000.2011.1.1.txt 0.1153367622354327\n",
      "filtered-41420.000.2017.1.1.txt 0.14503192800425604\n",
      "filtered-41320.000.2009.1.1.txt 0.15078857912107868\n",
      "filtered-42421.000.2008.1.1.txt 0.17893460011659063\n",
      "filtered-42422.000.2013.1.1.txt 0.19350410437145782\n",
      "filtered-41420.000.2009.1.1.txt 0.19945922658573473\n",
      "filtered-41950.000.2009.1.1.txt 0.19978270761864989\n",
      "filtered-41950.000.2013.1.1.txt 0.2058204031416493\n",
      "filtered-41523.000.2017.1.1.txt 0.20939602578569308\n",
      "filtered-43811.000.2011.1.1.txt 0.2188558441318758\n",
      "filtered-41953.000.2013.1.1.txt 0.22297287570478297\n",
      "filtered-41223.000.2017.1.1.txt 0.22453368947999658\n",
      "filtered-42450.000.2013.1.1.txt 0.22581307131989323\n",
      "filtered-42320.000.2008.1.1.txt 0.22827263900099654\n",
      "filtered-42951.000.2008.1.1.txt 0.2405620102556601\n",
      "filtered-41113.000.2017.1.1.txt 0.2791220194532305\n",
      "filtered-42420.000.2017.1.1.txt 0.28294968524885444\n",
      "filtered-43320.000.2007.1.1.txt 0.2890438612688067\n",
      "filtered-42520.000.2008.1.1.txt 0.2926725663076543\n",
      "filtered-41521.000.2013.1.1.txt 0.2991700894928469\n",
      "filtered-41953.000.2017.1.1.txt 0.31439217671785136\n",
      "filtered-43954.000.2015.1.1.txt 0.3263077358400589\n",
      "filtered-43520.000.2015.1.1.txt 0.3276393549633758\n",
      "filtered-43520.000.2007.1.1.txt 0.3335916231444039\n",
      "filtered-41521.000.2017.1.1.txt 0.33708261084232205\n",
      "filtered-43420.000.2007.1.1.txt 0.337174317902228\n",
      "filtered-42952.000.2008.1.1.txt 0.34450732746171797\n",
      "filtered-42520.000.2017.1.1.txt 0.3775855186020593\n",
      "filtered-41320.000.2013.1.1.txt 0.37856060617344495\n",
      "filtered-42520.000.2013.1.1.txt 0.38880341111959227\n",
      "filtered-43530.000.2015.1.1.txt 0.3979378472513959\n",
      "filtered-43320.000.2015.1.1.txt 0.40370586943515585\n",
      "filtered-42420.000.2008.1.1.txt 0.40479037523425787\n",
      "filtered-43530.000.2007.1.1.txt 0.4106346272574711\n",
      "filtered-43711.000.2011.1.1.txt 0.413485939429936\n",
      "filtered-42450.000.2017.1.1.txt 0.41540997452641804\n",
      "filtered-42110.000.2008.1.1.txt 0.4187372133277518\n",
      "filtered-43420.000.2015.1.1.txt 0.43351156823283093\n",
      "filtered-43531.000.2007.1.1.txt 0.43839889331345683\n",
      "filtered-41113.000.2009.1.1.txt 0.47292879654424924\n",
      "filtered-42110.000.2013.1.1.txt 0.4738979760094097\n",
      "filtered-42110.000.2017.1.1.txt 0.47769425703018203\n",
      "filtered-41521.000.2009.1.1.txt 0.48054483774856005\n",
      "filtered-43420.000.2011.1.1.txt 0.4867618322417745\n",
      "filtered-41113.000.2013.1.1.txt 0.48726638643805875\n",
      "filtered-41702.000.2009.1.1.txt 0.48840438775865636\n",
      "filtered-43110.000.2007.1.1.txt 0.4892449899593641\n",
      "filtered-43530.000.2011.1.1.txt 0.49201913623455446\n",
      "filtered-43953.000.2007.1.1.txt 0.4939694415989419\n",
      "filtered-41320.000.2017.1.1.txt 0.5146675266995772\n",
      "filtered-41702.000.2013.1.1.txt 0.531126802210552\n",
      "filtered-43110.000.2011.1.1.txt 0.5504682536031692\n",
      "filtered-43320.000.2011.1.1.txt 0.5661110084380048\n",
      "filtered-42320.000.2017.1.1.txt 0.6024682683483761\n",
      "filtered-43810.000.2011.1.1.txt 0.6048215966657151\n",
      "filtered-41420.000.2013.1.1.txt 0.6265373112167674\n",
      "filtered-42220.000.2017.1.1.txt 0.6699156665966909\n",
      "filtered-42420.000.2013.1.1.txt 0.6987600084786706\n",
      "filtered-42220.000.2013.1.1.txt 0.717569568949276\n",
      "filtered-41223.000.2009.1.1.txt 0.7195161533559827\n",
      "filtered-41223.000.2013.1.1.txt 0.7321919856661049\n",
      "filtered-43810.000.2015.1.1.txt 0.8810031684525308\n",
      "filtered-43220.000.2015.1.1.txt 0.8968802662746752\n",
      "filtered-43110.000.2015.1.1.txt 0.9168687333624859\n",
      "filtered-43810.000.2007.1.1.txt 1.0\n"
     ]
    }
   ],
   "source": [
    "scaling = open(out_file,\"r\").read().strip().split(\"\\n\")\n",
    "scaling = [x.split() for x in scaling]\n",
    "scaling.sort(key=lambda x: x[1])\n",
    "for el in scaling:\n",
    "    print (\" \".join(el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
